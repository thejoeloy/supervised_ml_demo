{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f882284f-7c27-4734-8891-848b4faf34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = []\n",
    "\n",
    "with open(\"housing.xls\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        A.append(row)\n",
    "\n",
    "A = np.array([list(map(float, row[0].split())) for row in A])\n",
    "\n",
    "X = A[:, :13]\n",
    "Y = A[:, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9f18b-fdd1-458c-8d48-08f041c43e65",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1671766c-0bda-49b6-8e05-aab4e183895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly Chosen Entries Evaluation:\n",
      "RMSE: 3.7625171274024587\n",
      "R-squared (R2): 0.8253575417005796\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "A = []\n",
    "\n",
    "with open(\"housing.xls\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        A.append(row)\n",
    "\n",
    "A = np.array([list(map(float, row[0].split())) for row in A])\n",
    "\n",
    "X = A[:, :13]\n",
    "Y = A[:, 13]\n",
    "\n",
    "# Define a function to train and evaluate the model on random entries\n",
    "def evaluate_random_entries(X, Y, num_entries=10, random_seed=42):\n",
    "    # Seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Step 1: Split the dataset into training and testing sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "    # Step 2: Calculate the coefficients using the closed-form solution (normal equation)\n",
    "    ones_column = np.ones((X_train.shape[0], 1))\n",
    "    X_train = np.concatenate((ones_column, X_train), axis=1)\n",
    "    coefficients = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
    "\n",
    "    # Step 3: Randomly select entries from the testing set\n",
    "    random_indices = np.random.choice(X_test.shape[0], num_entries, replace=False)\n",
    "    selected_X = X_test[random_indices]\n",
    "    selected_Y = Y_test[random_indices]\n",
    "\n",
    "    # Step 4: Make predictions on the selected entries using the trained model\n",
    "    ones_column = np.ones((selected_X.shape[0], 1))\n",
    "    selected_X = np.concatenate((ones_column, selected_X), axis=1)\n",
    "    predictions = selected_X @ coefficients\n",
    "\n",
    "    # Step 5: Evaluate the model's performance on the selected entries\n",
    "    rmse = np.sqrt(mean_squared_error(selected_Y, predictions))\n",
    "    r2 = r2_score(selected_Y, predictions)\n",
    "\n",
    "    return rmse, r2\n",
    "\n",
    "# Evaluate the model on 10 randomly chosen entries (you can change the number)\n",
    "rmse, r2 = evaluate_random_entries(X, Y, num_entries=10)\n",
    "\n",
    "print(\"Randomly Chosen Entries Evaluation:\")\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841b9f2-103e-4496-99c0-06da59cb132f",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "089b54ac-bd4a-4a16-a47e-7d37639de7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.0992748928062577\n",
      "Slopes: [-0.09074206  0.0660451  -0.01754142  2.08527767 -0.85121259  4.36797018\n",
      "  0.01980169 -0.77717413  0.1656636  -0.00922756 -0.13456081  0.01710921\n",
      " -0.54337423]\n",
      "Randomly Chosen Entries Evaluation:\n",
      "RMSE: 3.645828361371956\n",
      "R-squared (R2): 0.6104044704028384\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your data as you've done\n",
    "# Assuming you already have X (feature matrix) and Y (target values)\n",
    "\n",
    "# Step 1: Add a column of ones to X for the intercept term\n",
    "ones_column = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((ones_column, X), axis=1)\n",
    "\n",
    "# Step 2: Initialize coefficients with small random values (including intercept)\n",
    "np.random.seed(0)\n",
    "l_coefficients = np.random.randn(X.shape[1])\n",
    "\n",
    "# Hyperparameter for Lasso regularization (lambda)\n",
    "alpha = 0.0001\n",
    "\n",
    "# Number of iterations, learning rate, and scaling factor\n",
    "num_iterations = 1000000\n",
    "learning_rate = 0.000001\n",
    "scaling_factor = 1 / len(Y)\n",
    "\n",
    "# Step 3: Define the Lasso cost function\n",
    "def lasso_cost(X, Y, coefficients, alpha):\n",
    "    predictions = X.dot(coefficients)\n",
    "    error = Y - predictions\n",
    "    l1_penalty = alpha * np.sum(np.abs(coefficients[1:]))  # Exclude intercept from regularization\n",
    "    cost = np.mean(error**2) + l1_penalty\n",
    "    return cost\n",
    "\n",
    "# Step 4: Implement gradient descent\n",
    "for _ in range(num_iterations):\n",
    "    predictions = X.dot(l_coefficients)\n",
    "    error = Y - predictions\n",
    "    \n",
    "    # Compute the gradient of the cost function with respect to coefficients\n",
    "    gradient = -2 * X.T.dot(error) * scaling_factor\n",
    "    \n",
    "    # Update coefficients with Lasso regularization\n",
    "    l_coefficients[1:] -= learning_rate * (gradient[1:] + alpha * np.sign(l_coefficients[1:]))\n",
    "    l_coefficients[0] -= learning_rate * gradient[0]\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Intercept:\", l_coefficients[0])\n",
    "print(\"Slopes:\", l_coefficients[1:])\n",
    "\n",
    "# Now, let's evaluate the model's performance on random entries from the dataset\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Randomly select a few entries from the testing set\n",
    "random_indices = np.random.choice(X_test.shape[0], 10, replace=False)\n",
    "selected_X = X_test[random_indices]\n",
    "selected_Y = Y_test[random_indices]\n",
    "\n",
    "# Step 7: Make predictions on the selected entries using the trained model\n",
    "selected_predictions = selected_X.dot(l_coefficients)\n",
    "\n",
    "# Step 8: Evaluate the model's performance on these selected entries\n",
    "rmse = np.sqrt(mean_squared_error(selected_Y, selected_predictions))\n",
    "r2 = r2_score(selected_Y, selected_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Randomly Chosen Entries Evaluation:\")\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dc3d4-9a33-40f2-a9b2-6620bb5c29c2",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "035b2128-aed3-4f29-b1a9-d1e102907990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.1576501409990083\n",
      "Slopes: [ 0.79250438 -0.09371877  0.06813992 -0.0046147  -0.41352885  1.10003442\n",
      "  4.06416296  0.02610843 -0.6989206   0.18739186 -0.01073717 -0.13891769\n",
      "  0.0177176  -0.58065627]\n",
      "Randomly Chosen Entries Evaluation:\n",
      "RMSE: 2.619446056963898\n",
      "R-squared (R2): 0.5896233465703522\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your data as you've done\n",
    "# Assuming you already have X (feature matrix) and Y (target values)\n",
    "\n",
    "# Step 1: Add a column of ones to X for the intercept term\n",
    "ones_column = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((ones_column, X), axis=1)\n",
    "\n",
    "# Step 2: Initialize coefficients with small random values (including intercept)\n",
    "np.random.seed(0)\n",
    "r_coefficients = np.random.randn(X.shape[1])\n",
    "\n",
    "# Hyperparameter for Ridge regularization (lambda)\n",
    "alpha = 0.001\n",
    "\n",
    "# Number of iterations, learning rate, and scaling factor\n",
    "num_iterations = 1000000\n",
    "learning_rate = 0.000001\n",
    "scaling_factor = 1 / len(Y)\n",
    "\n",
    "# Step 3: Define the Ridge cost function\n",
    "def ridge_cost(X, Y, coefficients, alpha):\n",
    "    predictions = X.dot(coefficients)\n",
    "    error = Y - predictions\n",
    "    l2_penalty = alpha * np.sum(coefficients[1:]**2)  # Exclude intercept from regularization\n",
    "    cost = np.mean(error**2) + l2_penalty\n",
    "    return cost\n",
    "\n",
    "# Step 4: Implement gradient descent\n",
    "for _ in range(num_iterations):\n",
    "    predictions = X.dot(r_coefficients)\n",
    "    error = Y - predictions\n",
    "    \n",
    "    # Compute the gradient of the cost function with respect to coefficients\n",
    "    gradient = -2 * X.T.dot(error) * scaling_factor\n",
    "    \n",
    "    # Update coefficients with Ridge regularization\n",
    "    r_coefficients[1:] -= learning_rate * (gradient[1:] + 2 * alpha * r_coefficients[1:])\n",
    "    r_coefficients[0] -= learning_rate * gradient[0]\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Intercept:\", r_coefficients[0])\n",
    "print(\"Slopes:\", r_coefficients[1:])\n",
    "\n",
    "# Now, let's evaluate the model's performance on random entries from the dataset\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Randomly select a few entries from the testing set\n",
    "random_indices = np.random.choice(X_test.shape[0], 10, replace=False)\n",
    "selected_X = X_test[random_indices]\n",
    "selected_Y = Y_test[random_indices]\n",
    "\n",
    "# Step 7: Make predictions on the selected entries using the trained model\n",
    "selected_predictions = selected_X.dot(r_coefficients)\n",
    "\n",
    "# Step 8: Evaluate the model's performance on these selected entries\n",
    "rmse = np.sqrt(mean_squared_error(selected_Y, selected_predictions))\n",
    "r2 = r2_score(selected_Y, selected_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Randomly Chosen Entries Evaluation:\")\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55317c2-c2a4-4770-878f-428c22ad8664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
